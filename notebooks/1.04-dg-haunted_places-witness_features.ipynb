{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.04 - Witness Feature Extraction\n",
    "## **Haunted Places Witness Count**\n",
    "\n",
    "Numbers extacted using [numberscraper](https://github.com/scrapinghub/number-parser)\n",
    "\n",
    "**\"Haunted_Places_Witness_Count\" [datetime]**\n",
    "- Format: int\n",
    "- Default Value: 0\n",
    "\n",
    "**NOTES**:\n",
    "- Considerations:\n",
    "    - Multiple witness accounts in the same entry (sum)?\n",
    "    - How many is \"Several\"?\n",
    "- Regex using pronouns\n",
    "    - \"I\", \"we\", \"me\"\n",
    "    - \"Several\", \"some\", \"they\"\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Path #\n",
    "import os\n",
    "import sys \n",
    "\n",
    "# Pandas #\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "\n",
    "# Number Parser #\n",
    "from number_parser import parse_ordinal\n",
    "from number_parser import parse_number\n",
    "from number_parser import parse\n",
    "\n",
    "# Tika #\n",
    "import tika as tk\n",
    "from tika import parser\n",
    "\n",
    "\n",
    "\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "\n",
    "# Reading CSV\n",
    "df = pd.read_csv(\"../data/processed/haunted_places_cleaned.tab\", sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Whitness Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "\n",
    "# Download necessary resources (only need to run once)\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "def extract_nouns(text):\n",
    "    # Tokenize text\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Get POS tags\n",
    "    tagged_words = pos_tag(words)\n",
    "    \n",
    "    # Filter nouns: NN (singular), NNS (plural), NNP (proper noun, singular), NNPS (proper noun, plural)\n",
    "    nouns = [word for word, tag in tagged_words if tag in [\"NN\", \"NNS\", \"NNP\", \"NNPS\"]]\n",
    "    \n",
    "    return nouns\n",
    "\n",
    "\n",
    "noun_counts = Counter()\n",
    "# Sample text\n",
    "for idx in range(df.shape[0]):\n",
    "    sequence = df['description'][idx].split()\n",
    "    tagged_sequence = pos_tag(sequence)\n",
    "    for word, tag in tagged_sequence:\n",
    "        if tag in [\"NN\", \"NNS\", \"NNP\", \"NNPS\"]:\n",
    "            noun_counts[word.lower()] += 1\n",
    "    \n",
    "        \n",
    "quantifer_counts = Counter()\n",
    "for idx in range(df.shape[0]):\n",
    "    sequence = df['description'][idx].split()\n",
    "    tagged_sequence = pos_tag(sequence)\n",
    "    for word, tag in tagged_sequence:\n",
    "        if tag in [\"DT\", \"PDT\"]:\n",
    "            quantifer_counts[word.lower()] += 1\n",
    "    \n",
    "noun_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing First Person\n",
    "\n",
    "test case:\n",
    "- idx: 501\n",
    "\n",
    "    told see light changes colors red white floats around one ever found unsolved mysteries could find i many time seen story older man went save two kids train pushed way got decapitated wonders woods looking head holding lantern\n",
    "    - goal : 1\n",
    "- idx: 761\n",
    "\n",
    "    said area still used kkk bridge said people killed bridge put keys bridge 5 mins car start road bad tho really hilly also winter maintance alarmed we recentaly walkedup hill looks like path said top meeting ground we walking path i turned light camera woods we seen white figure we stick around long enough investigate october 2004 correction family died bridge early setlers area late 1700 's county began settled reported mother frantically looking childen underbrush hear rustling bushes plaintive call distance reported carries glowing taper lantern\n",
    "    - goal : 2\n",
    "- idx: 876\n",
    "\n",
    "    strange noises figures seen i seen heard doors slam shut lights flicker early hours morning rooms become chillingly cold also enter building get feeling followed even though one around\n",
    "    - goal : 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from itertools import chain\n",
    "from dsci_550_a1.parsingFunctions import extractSequences\n",
    "\n",
    "# Verb Dictionary\n",
    "with open(\"../data/keywords/witness_verbs.json\", \"r\") as file:\n",
    "    witness_verbs = json.load(file)\n",
    "\n",
    "\n",
    "## First Person Regex Check\n",
    "\n",
    "def first_person_regex_check(text):\n",
    "\n",
    "    ## Extract sequences\n",
    "    tokens = text.split()\n",
    "    sequences = extractSequences(tokens, '.')\n",
    "\n",
    "    ## Regex checks for \"we\" and \"i\"\n",
    "    pattern = r\"\\bi\\b|\\bwe\\b\" \n",
    "\n",
    "    ## Init verb set ##\n",
    "    verb_set = set(chain(*witness_verbs.values())) \n",
    "\n",
    "    ## Iterate through sequences\n",
    "    for sequence in sequences:\n",
    "        ## Iterate though tokens in sequence\n",
    "        for idx, token in enumerate(sequence):\n",
    "            \n",
    "            # If \"i\" or \"we\" found\n",
    "            if re.search(pattern, token, re.IGNORECASE):\n",
    "                try:\n",
    "                    # Check rest of sentence for overlap \n",
    "                    if any(word in verb_set for word in sequence[idx:]):\n",
    "\n",
    "                        # First person Plural\n",
    "                        if token == \"we\":\n",
    "                            return 2\n",
    "                        \n",
    "                        # First person singular\n",
    "                        elif token == 'i':\n",
    "                            return 1\n",
    "\n",
    "\n",
    "                except IndexError:\n",
    "                    pass\n",
    "    return 0\n",
    "\n",
    "\n",
    "test_cases = [(501, 1), (761, 2), (876,1)]\n",
    "\n",
    "\n",
    "for index,target in test_cases:\n",
    "    description = df['description'][index]\n",
    "    extracted_witnesses = first_person_regex_check(description)\n",
    "    print(\"-\"*50, f\"Test_Case: {index}\", description, f\"Target Witnesses: {target} | Extracted Witnesses: {extracted_witnesses}\", \"-\"*50, sep = \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "df['Haunted_Places_Witness_Count'] = df['description'].apply(first_person_regex_check)\n",
    "df['Haunted_Places_Witness_Count'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing Ambiguous Quantifiers\n",
    "- idx: 1012\n",
    "\n",
    "    rumored ghost edgar allan poe still exists eutaw house restaurant historic inn/restaurant central pa history hauntings researching ghost stories pa article mother i went talk  haunting  shortly we left horse bells dining area door began ring **several people reported** feeling `` unusually uncomfortable watched '' went upstairs restrooms\n",
    "    - goal : 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"rumored ghost edgar allan poe still exists eutaw house restaurant . historic inn/restaurant central pa history hauntings . researching ghost stories pa article mother i went talk `` haunting '' . shortly we left horse bells dining area door began ring 3 people reported feeling `` unusually uncomfortable watched '' went upstairs restrooms .\""
      ]
     },
     "execution_count": 659,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "quantifiers = {\n",
    "    \"2\" : ['pair', 'couple'], \n",
    "    \"3\" : ['many', 'several', 'some', 'few', 'group', 'groups']\n",
    "}\n",
    "quantifiers = {word: number for number, words in quantifiers.items() for word in words}\n",
    "\n",
    "quantifiers_regex = {key : re.compile(r\"\\b(\" + \"|\".join(map(re.escape, value)) + r\")\\b\", re.IGNORECASE) for key, value in quantifiers.items()}\n",
    "\n",
    "def parse_ambiguous(text: str) -> str:\n",
    "    '''\n",
    "    Parses Ambiguous quantifiers like \"several\" and \"many\" and replaces them with numbers\n",
    "    Input:\n",
    "        [text]  - raw text\n",
    "    Returns:\n",
    "        [text]  - text with quantifiers replaced with numbers\n",
    "    '''\n",
    "    # Parse Quantifiers #\n",
    "    tokens = text.split()\n",
    "    tokens = [quantifiers.get(token, token) for token in tokens]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "parse_ambiguous(df['description'][1012])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eyewitness Extraction (no sliding window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "witness_nouns = json.load(open(\"../data/keywords/witness_nouns.json\", \"r\"))\n",
    "witness_verbs = json.load(open(\"../data/keywords/witness_verbs.json\", \"r\"))\n",
    "\n",
    "def extract_eyewitness_counts(text: str) -> int:\n",
    "    '''\n",
    "    Extract number of witnesses from a block of text.\n",
    "\n",
    "    Steps:\n",
    "    1. Tokenize and sequence text\n",
    "    2. Identify Witness-Specific Nouns\n",
    "    3. Identify Witness-Specific Verbs\n",
    "    4. Check for previous quantifier\n",
    "    5. +1 for singular, +3 for plural, or +quantifier\n",
    "\n",
    "    Input:\n",
    "        [text]            - raw text with quantifiers converted to digits\n",
    "    Returns:\n",
    "        [witness_counts]  - Number of witnesses\n",
    "\n",
    "    eg: \n",
    "    >>> extract_eyewitness_counts(\"2 girls names elizabeth evelyn felt legs pulled sitting bleachers.\")\n",
    "    2\n",
    "    '''\n",
    "    ## Extract Tokens ##\n",
    "    tokens = text.split()\n",
    "\n",
    "    ## Extract Sequences ##\n",
    "    sequences = extractSequences(tokens, '.')\n",
    "\n",
    "    ## Noun Regex Patterns. See witness_nouns.josn ##\n",
    "    singular_noun_pattern = re.compile(r\"\\b(\" + \"|\".join(map(re.escape, list(chain(*witness_nouns['Singular'].values())))) + r\")\\b\", re.IGNORECASE)\n",
    "    plural_noun_pattern = re.compile(r\"\\b(\" + \"|\".join(map(re.escape, list(chain(*witness_nouns['Plural'].values())))) + r\")\\b\", re.IGNORECASE)\n",
    "    \n",
    "    ## Singular Nouns \n",
    "    regex_dict = {\n",
    "        '1' : singular_noun_pattern,\n",
    "        '3' : plural_noun_pattern\n",
    "    }\n",
    "    ## Verb Set ##\n",
    "    verb_set = set(chain(*witness_verbs.values())) \n",
    "\n",
    "    ## Initialize Witness Count ##\n",
    "    witness_count = 0 \n",
    "\n",
    "    for sequence in sequences:\n",
    "\n",
    "        ## Check Singular and Plural Patterns ##\n",
    "        for val, regex_pattern, in regex_dict.items():\n",
    "        \n",
    "            default_value = int(val)\n",
    "\n",
    "            ## Iterate through each sentence ##\n",
    "            for idx, token in enumerate(sequence):\n",
    "\n",
    "                # If noun match found \n",
    "                if regex_pattern.match(token):\n",
    "\n",
    "                    # Check rest of sequence for verb \n",
    "                    if any(word in verb_set for word in sequence[idx:]):\n",
    "\n",
    "                        ## Check for previous quantifier ## \n",
    "                        prev_token = sequence[idx - 1] if idx > 0 else None\n",
    "\n",
    "                        if prev_token and prev_token.isdigit():\n",
    "                            witness_count += int(prev_token)\n",
    "                            print(token)\n",
    "                        else:\n",
    "                            print(token)\n",
    "                            witness_count += default_value\n",
    "                    \n",
    "        \n",
    "    return witness_count\n",
    "                    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eyewitness Extraction (with sliding window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 670,
   "metadata": {},
   "outputs": [],
   "source": [
    "witness_nouns = json.load(open(\"../data/keywords/witness_nouns.json\", \"r\"))\n",
    "witness_verbs = json.load(open(\"../data/keywords/witness_verbs.json\", \"r\"))\n",
    "\n",
    "def extract_eyewitness_counts_slide(text: str) -> int:\n",
    "    '''\n",
    "    Extract number of witnesses from a block of text using sliding window.\n",
    "\n",
    "    Steps:\n",
    "    1. Tokenize and sequence text\n",
    "    2. initialize sentence window\n",
    "    2. Identify Witness-Specific Nouns\n",
    "    3. If noun found, check next 3 sentences for witness-specific verbs\n",
    "    4. If verb found, check previous token for quantifier\n",
    "    5. Increment witness_count, +1 for singular, +3 for plural, or +quantifier\n",
    "    6. Move sliding window to sentence following witness-verb\n",
    "    7. loop until final sentence\n",
    "\n",
    "    Input:\n",
    "        [text]            - raw text with quantifiers converted to digits\n",
    "    Returns:\n",
    "        [witness_counts]  - Number of witnesses\n",
    "        [witnesses]       - token flagged as witness\n",
    "\n",
    "    eg: \n",
    "    >>> extract_eyewitness_counts(\"2 girls names elizabeth evelyn felt legs pulled sitting bleachers.\")\n",
    "    (2, [['girls', 2]])\n",
    "    '''\n",
    "    \n",
    "    ## Extract Tokens ##\n",
    "    tokens = text.replace('.', ' . ').split()\n",
    "\n",
    "    ## Extract Sequences ##\n",
    "    sequences = extractSequences(tokens, '.')\n",
    "\n",
    "    ## Noun Regex Patterns. See witness_nouns.josn ##\n",
    "    singular_noun_pattern = re.compile(r\"\\b(\" + \"|\".join(map(re.escape, list(chain(*witness_nouns['Singular'].values())))) + r\")\\b\", re.IGNORECASE)\n",
    "    plural_noun_pattern = re.compile(r\"\\b(\" + \"|\".join(map(re.escape, list(chain(*witness_nouns['Plural'].values())))) + r\")\\b\", re.IGNORECASE)\n",
    "    \n",
    "    ## Singular Nouns \n",
    "    regex_dict = {\n",
    "        '1' : singular_noun_pattern,\n",
    "        '2' : plural_noun_pattern\n",
    "    }\n",
    "    ## Verb Set ##\n",
    "    verb_set = set(chain(*witness_verbs.values())) \n",
    "\n",
    "    ## Initialize Witness Count and witness list##\n",
    "    witnesses = []\n",
    "    witness_count = 0 \n",
    "    num_sequences = len(sequences)\n",
    "\n",
    "    i = 0 \n",
    "    while i < num_sequences:\n",
    "        starting_sequence = sequences[i]\n",
    "\n",
    "        ## Check First Person Regex ##\n",
    "        first_person_witnesses = first_person_regex_check(\" \".join(starting_sequence))\n",
    "        if first_person_witnesses != 0:\n",
    "            witness_count += first_person_witnesses\n",
    "            witnesses.append(['i|we', '1|2'])\n",
    "            i += 1\n",
    "            break\n",
    "\n",
    "        \n",
    "        ## Check Singular and Plural Patterns ##\n",
    "        for val, regex_pattern, in regex_dict.items():\n",
    "            \n",
    "            default_value = int(val)\n",
    "\n",
    "            ## Iterate through each sentence ##\n",
    "            for idx, token in enumerate(starting_sequence):\n",
    "                \n",
    "\n",
    "                # If noun match found \n",
    "                if regex_pattern.match(token):\n",
    "\n",
    "                    ## Search for verb within 3 sentence window\n",
    "                    for j in range(i, min(i + 2, num_sequences)):\n",
    "                        ending_sequence = sequences[j] \n",
    "\n",
    "                        # If verb found, check starting sentence for quantifier\n",
    "                        if any(word in verb_set for word in ending_sequence):\n",
    "\n",
    "                            prev_token = starting_sequence[idx -1] if idx > 0 else None\n",
    "\n",
    "                            # If digit, add value and set sliding window to start at next sentence. \n",
    "                            # Filter quantifiers over 15\n",
    "                            if prev_token and prev_token.isdigit() and int(prev_token) < 16:\n",
    "                                witness_count += int(prev_token)\n",
    "                                i = min(j+1, num_sequences)\n",
    "                                witnesses.append([token, int(prev_token)])\n",
    "                                break\n",
    "\n",
    "                            # If no quantifier, add default_value and set sliding window to start at next sentence\n",
    "                            else:\n",
    "                                witness_count += default_value\n",
    "                                i = min(j+1, num_sequences)\n",
    "                                witnesses.append([token,default_value])\n",
    "                                break\n",
    "\n",
    "                    ## Break loop once witness is added or no verb match found in 3 sentences. ##\n",
    "                    ## Begin loop from new starting_sentence ##\n",
    "                    break\n",
    "            \n",
    "                \n",
    "        ## Start at next sentence if loop fails ##\n",
    "        i += 1\n",
    "    return witness_count, witnesses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_cases = [[1176, 16], [1185,3], [1237,1], [1412,2], [1426, 0], [1428, 0], [501, 1], [761, 2], [876,1]]\n",
    "\n",
    "for index,target in test_cases:\n",
    "    description = df['description'][index]\n",
    "    description = parse(parse_ambiguous(description))\n",
    "    extracted_witnesses, witnesses = extract_eyewitness_counts_slide(description)\n",
    "    print(\"-\"*50, f\"Test_Case: {index}\", description, f\"Target Witnesses: {target} | Extracted Witnesses: {extracted_witnesses}\", f\"flagged tokens: {witnesses}\", \"-\"*50, sep = \"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parse Quantifiers \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10991/10991 [00:17<00:00, 613.97it/s]\n",
      "100%|██████████| 10991/10991 [00:00<00:00, 201448.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------------------------------------ Quantifiers Parsed ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "'number scraper' runtime: 17.904596 seconds\n",
      "'parse_ambiguous' runtime: 0.056002 seconds\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm.pandas()\n",
    "\n",
    "# Apply the first method and save as a variable\n",
    "t1 = time.time()\n",
    "parsed_decriptions = df[\"description\"].progress_apply(parse)\n",
    "t2 = time.time()\n",
    "# Apply the second method on the intermediate result\n",
    "parsed_descriptions = parsed_decriptions.progress_apply(parse_ambiguous)\n",
    "t3 = time.time()\n",
    "\n",
    "print(\"-\" * 150, \"Quantifiers Parsed\", \"-\" * 150)\n",
    "print(f\"'number scraper' runtime: {t2 - t1:.6f} seconds\", end = \"\\n\")\n",
    "print(f\"'parse_ambiguous' runtime: {t3 - t2:.6f} seconds\", end = \"\\n\\n\")\n",
    "print(\"-\" * 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Entries: 100%|██████████| 10991/10991 [00:01<00:00, 6940.73it/s]\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "df[\"Haunted_Places_Witness_Count\"] = [\n",
    "    extract_eyewitness_counts_slide((entry))\n",
    "    for entry in tqdm(parsed_decriptions, desc = \"Processing Entries\")\n",
    "]\n",
    "end = time.time()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Unpack Outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Witness Names\n",
    "df[\"Haunted_Places_Witnesses\"] = df[\"Haunted_Places_Witness_Count\"].apply(lambda x: x[1])\n",
    "# Witness Counts\n",
    "df[\"Haunted_Places_Witness_Count\"] = df[\"Haunted_Places_Witness_Count\"].apply(lambda x: x[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 682,
   "metadata": {},
   "outputs": [],
   "source": [
    "witness_name_counter = Counter()\n",
    "for entry in df[\"Haunted_Places_Witnesses\"]:\n",
    "    for witness, _ in entry:\n",
    "        witness_name_counter[witness] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------------------------------------------------------------------------------------------------------ Extraction Completed ------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Extraction Took: 1.587603 seconds\n",
      "\n",
      "Value Counts\n",
      "Haunted_Places_Witness_Count\n",
      "0     4110\n",
      "1     3263\n",
      "2     2348\n",
      "3      764\n",
      "4      310\n",
      "5      101\n",
      "6       53\n",
      "7       18\n",
      "8        7\n",
      "9        7\n",
      "13       3\n",
      "12       3\n",
      "16       1\n",
      "15       1\n",
      "19       1\n",
      "10       1\n",
      "Name: count, dtype: int64\n",
      "approximate coverage: 0.6260576835592758\n",
      "\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Haunted Places Witnesses\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "10 Most Common Witness Names\n",
      "[('people', 1246), ('man', 878), ('girl', 674), ('woman', 648), ('someone', 351), ('boy', 321), ('children', 287), ('lady', 234), ('students', 211), ('employees', 205)]\n",
      "10 Least Common Witness Names\n",
      "[('gentlemen', 1), ('couples', 1), ('males', 1), ('teacher\\\\director', 1), ('children-', 1), ('woman-like', 1), ('daughter-in-law', 1), ('soldier-like', 1), ('sheriffs', 1), ('man-', 1)]\n",
      "------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"-\" * 150, \"Extraction Completed\", \"-\" * 150)\n",
    "print(f\"Extraction Took: {end - start:.6f} seconds\", end = \"\\n\\n\")\n",
    "print(\"Value Counts\", df[\"Haunted_Places_Witness_Count\"].value_counts(), sep = \"\\n\")\n",
    "print(f\"approximate coverage: {(df['Haunted_Places_Witness_Count'] != 0).sum() / df.shape[0]}\")\n",
    "print(f\"\")\n",
    "print(\"-\" * 150)\n",
    "print(\"Haunted Places Witnesses\", \"-\" * 150, sep = \"\\n\")\n",
    "print(\"10 Most Common Witness Names:\", witness_name_counter.most_common(10), sep = \"\\n\")\n",
    "print(\"10 Least Common Witness Names:\", witness_name_counter.most_common()[-11:-1], sep = \"\\n\")\n",
    "print(\"-\" * 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
