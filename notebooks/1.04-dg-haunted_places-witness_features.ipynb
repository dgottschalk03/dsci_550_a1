{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.04 - Witness Feature Extraction\n",
    "## **Haunted Places Witness Count**\n",
    "\n",
    "Numbers extacted using [numberscraper](https://github.com/scrapinghub/number-parser)\n",
    "\n",
    "**\"Haunted_Places_Witness_Count\" [datetime]**\n",
    "- Format: int\n",
    "- Default Value: 0\n",
    "\n",
    "**NOTES**:\n",
    "- Considerations:\n",
    "    - Multiple witness accounts in the same entry (sum)?\n",
    "    - How many is \"Several\"?\n",
    "- Regex using pronouns\n",
    "    - \"I\", \"we\", \"me\"\n",
    "    - \"Several\", \"some\", \"they\"\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System Path #\n",
    "import os\n",
    "import sys \n",
    "\n",
    "# Pandas #\n",
    "import pandas as pd\n",
    "import time\n",
    "import re\n",
    "\n",
    "# Number Parser #\n",
    "from number_parser import parse_ordinal\n",
    "from number_parser import parse_number\n",
    "from number_parser import parse\n",
    "\n",
    "# Tika #\n",
    "import tika as tk\n",
    "from tika import parser\n",
    "\n",
    "\n",
    "\n",
    "parent_dir = os.path.abspath(os.path.join(os.getcwd(), \"..\"))\n",
    "sys.path.append(parent_dir)\n",
    "\n",
    "\n",
    "# Reading CSV\n",
    "df = pd.read_csv(\"../data/processed/haunted_places_cleaned.tab\", sep = \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "headers = df.columns.tolist()\n",
    "num_rows = df.shape[0]\n",
    "column_number = headers.index(\"description\") + 1\n",
    "\n",
    "# Test cases are (entry_number, expected_output)\n",
    "test_cases = [[1177, 16], [1186,3], [1238,1], [1413,3], [1427, 0], [1428, 0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['description'][561]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Whitness Nouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from collections import Counter\n",
    "\n",
    "# Download necessary resources (only need to run once)\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "# nltk.download('averaged_perceptron_tagger_eng')\n",
    "\n",
    "def extract_nouns(text):\n",
    "    # Tokenize text\n",
    "    words = word_tokenize(text)\n",
    "    \n",
    "    # Get POS tags\n",
    "    tagged_words = pos_tag(words)\n",
    "    \n",
    "    # Filter nouns: NN (singular), NNS (plural), NNP (proper noun, singular), NNPS (proper noun, plural)\n",
    "    nouns = [word for word, tag in tagged_words if tag in [\"NN\", \"NNS\", \"NNP\", \"NNPS\"]]\n",
    "    \n",
    "    return nouns\n",
    "\n",
    "\n",
    "noun_counts = Counter()\n",
    "# Sample text\n",
    "for idx in range(df.shape[0]):\n",
    "    sequence = df['description'][idx].split()\n",
    "    tagged_sequence = pos_tag(sequence)\n",
    "    for word, tag in tagged_sequence:\n",
    "        if tag in [\"NN\", \"NNS\", \"NNP\", \"NNPS\"]:\n",
    "            noun_counts[word.lower()] += 1\n",
    "    \n",
    "        \n",
    "quantifer_counts = Counter()\n",
    "for idx in range(df.shape[0]):\n",
    "    sequence = df['description'][idx].split()\n",
    "    tagged_sequence = pos_tag(sequence)\n",
    "    for word, tag in tagged_sequence:\n",
    "        if tag in [\"DT\", \"PDT\"]:\n",
    "            quantifer_counts[word.lower()] += 1\n",
    "    \n",
    "noun_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing First Person\n",
    "\n",
    "test case:\n",
    "- idx: 501\n",
    "\n",
    "    told see light changes colors red white floats around one ever found unsolved mysteries could find i many time seen story older man went save two kids train pushed way got decapitated wonders woods looking head holding lantern\n",
    "    - goal : 1\n",
    "- idx: 761\n",
    "\n",
    "    said area still used kkk bridge said people killed bridge put keys bridge 5 mins car start road bad tho really hilly also winter maintance alarmed we recentaly walkedup hill looks like path said top meeting ground we walking path i turned light camera woods we seen white figure we stick around long enough investigate october 2004 correction family died bridge early setlers area late 1700 's county began settled reported mother frantically looking childen underbrush hear rustling bushes plaintive call distance reported carries glowing taper lantern\n",
    "    - goal : 2\n",
    "- idx: 876\n",
    "\n",
    "    strange noises figures seen i seen heard doors slam shut lights flicker early hours morning rooms become chillingly cold also enter building get feeling followed even though one around\n",
    "    - goal : 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Haunted_Places_Witness_Count\n",
       "0    10550\n",
       "1      268\n",
       "3      173\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from itertools import chain\n",
    "# Verb Dictionary\n",
    "with open(\"../data/keywords/witness_verbs.json\", \"r\") as file:\n",
    "    witness_verbs = json.load(file)\n",
    "\n",
    "\n",
    "## First Person Regex Check\n",
    "\n",
    "def first_person_regex_check(text):\n",
    "    tokens = text.split()\n",
    "    # Checks for \"we\" and \"i\"\n",
    "    pattern = r\"\\bi\\b|\\bwe\\b|\\bme\\b\" \n",
    "    for idx, token in enumerate(tokens):\n",
    "        if re.search(pattern, token, re.IGNORECASE):\n",
    "            try:\n",
    "                # Check Verb Overlap #\n",
    "                overlap = set(tokens[idx:idx+5]).intersection(chain(*witness_verbs.values())) \n",
    "                if overlap != {}:\n",
    "                    # First Person Singular\n",
    "                    if token == \"we\":\n",
    "                        return 3\n",
    "                    # First Person Plural\n",
    "                    else:\n",
    "                        return 1\n",
    "            # Handles IndexError\n",
    "            except IndexError:\n",
    "                pass\n",
    "    return 0\n",
    "\n",
    "df['Haunted_Places_Witness_Count'] = df['description'].apply(first_person_regex_check)\n",
    "df['Haunted_Places_Witness_Count'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How Many is \"Several?\"\n",
    "- idx: 1012\n",
    "\n",
    "    rumored ghost edgar allan poe still exists eutaw house restaurant historic inn/restaurant central pa history hauntings researching ghost stories pa article mother i went talk  haunting  shortly we left horse bells dining area door began ring **several people reported** feeling `` unusually uncomfortable watched '' went upstairs restrooms\n",
    "    - goal : 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"rumored ghost edgar allan poe still exists eutaw house restaurant historic inn/restaurant central pa history hauntings researching ghost stories pa article mother i went talk `` haunting '' shortly we left horse bells dining area door began ring 3 people reported feeling `` unusually uncomfortable watched '' went upstairs restrooms\""
      ]
     },
     "execution_count": 297,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "quantifiers = {\n",
    "    \"2\" : ['pair', 'couple'], \n",
    "    \"3\" : ['many', 'several', 'some', 'few', 'group', 'groups']\n",
    "}\n",
    "\n",
    "quantifiers_regex = {key : re.compile(r\"\\b(\" + \"|\".join(map(re.escape, value)) + r\")\\b\", re.IGNORECASE) for key, value in quantifiers.items()}\n",
    "\n",
    "def parse_numbers(text):\n",
    "\n",
    "    # Parse Quantifiers #\n",
    "    tokens = text.split()\n",
    "    for idx, token in enumerate(tokens):\n",
    "        for number, regex in quantifiers_regex.items():\n",
    "            if regex.search(token):\n",
    "                tokens[idx] = number\n",
    "    # Number Parser #\n",
    "    tokens = [parse(token) for token in tokens]\n",
    "\n",
    "    # Text With Parsed Numbers#\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "parse_numbers(df['description'][1012])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Noun Parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "witness_nouns = json.load(open(\"../data/keywords/witness_nouns.json\", \"r\"))\n",
    "\n",
    "# parse nouns\n",
    "\n",
    "# parse numbers\n",
    "\n",
    "# parse singular pronouns\n",
    "test_cases = [[1177, 16], [1186,3], [1238,1], [1413,3], [1427, 0], [1428, 0]]\n",
    "\n",
    "def extract_eyewitness_counts(text):\n",
    "    tokens = text.split()\n",
    "    \n",
    "    noun_pattern = re.compile(r\"\\b(\" + \"|\".join(map(re.escape, list(chain(*witness_nouns.values())))) + r\")\\b\", re.IGNORECASE)\n",
    "    for idx, token in enumerate(tokens):\n",
    "        if noun_pattern.search(token):\n",
    "\n",
    "            verb_flag = set(tokens[idx:idx+5]).intersection(chain(*witness_verbs.values()))\n",
    "            if verb_flag != set():\n",
    "                \n",
    "                if tokens[idx - 1].isdigit():\n",
    "                    return int(tokens[idx -1])\n",
    "                else:\n",
    "                    if token.endswith('s'):\n",
    "                        return 2\n",
    "                    else:\n",
    "                        print(token)\n",
    "                        return 1\n",
    "                        \n",
    "\n",
    "\n",
    "test_str =  parse_numbers(df['description'][1412])     \n",
    "extract_eyewitness_counts(test_str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/dgottschalk/Desktop/SP_25/DSCI_550/Assignments/dsci_550_a1/notebooks'"
      ]
     },
     "execution_count": 341,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dsci_550_a1.parsingFunctions import extractSequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_eyewitness(text):\n",
    "    return parse(text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
